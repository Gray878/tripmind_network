#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TripMind ç½‘ç»œæŠ“å–åŠŸèƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯• Web Scraper Agent å’Œ Information Analyzer Agent çš„åŠŸèƒ½
"""

import json
import time
import sys
import os

# æ·»åŠ å·¥å…·è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'tools'))

from web_scraper import scrape_travel_info
from information_analyzer import analyze_information

def test_web_scraper():
    """æµ‹è¯•ç½‘ç»œæŠ“å–å™¨"""
    print("=" * 60)
    print("æµ‹è¯• 1: Web Scraper Agent åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•æ™¯ç‚¹æŠ“å–
    print("\nğŸ” æµ‹è¯•æ™¯ç‚¹ä¿¡æ¯æŠ“å–...")
    query = {
        'location': 'ä¸œäº¬',
        'keywords': ['æ–‡åŒ–', 'å†å²'],
        'budget_range': [0, 500],
        'preferences': ['åšç‰©é¦†', 'å…¬å›­']
    }
    
    result = scrape_travel_info('attractions', query)
    scraping_data = json.loads(result)
    
    if scraping_data['success']:
        print(f"âœ… æŠ“å–æˆåŠŸï¼è·å¾— {scraping_data['metadata']['total_items']} æ¡æ™¯ç‚¹ä¿¡æ¯")
        print(f"   æ•°æ®æº: {', '.join(scraping_data['metadata']['sources_used'])}")
        print(f"   è€—æ—¶: {scraping_data['metadata']['scraping_duration']} ç§’")
        
        # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
        print("\nğŸ“ æŠ“å–ç»“æœé¢„è§ˆ:")
        for i, item in enumerate(scraping_data['raw_data'][:3]):
            print(f"   {i+1}. {item['name']} ({item['type']})")
            print(f"      è¯„åˆ†: {item['rating']}/5.0")
            
            # å®‰å…¨å¤„ç†ä»·æ ¼å­—æ®µ
            if isinstance(item.get('price'), dict):
                print(f"      ä»·æ ¼: {item['price']['text']}")
            else:
                print(f"      ä»·æ ¼: {item.get('price', 'æœªçŸ¥')}")
            
            print(f"      æ ‡ç­¾: {', '.join(item['tags'])}")
    else:
        print(f"âŒ æŠ“å–å¤±è´¥: {scraping_data.get('error', 'æœªçŸ¥é”™è¯¯')}")
        return None
    
    return scraping_data['raw_data']

def test_information_analyzer(raw_data):
    """æµ‹è¯•ä¿¡æ¯åˆ†æå™¨"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: Information Analyzer Agent åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    if not raw_data:
        print("âŒ æ²¡æœ‰åŸå§‹æ•°æ®å¯ä¾›åˆ†æ")
        return
    
    print(f"\nğŸ§  å¼€å§‹åˆ†æ {len(raw_data)} æ¡åŸå§‹æ•°æ®...")
    
    result = analyze_information(raw_data, "comprehensive")
    analysis_data = json.loads(result)
    
    if analysis_data['success']:
        processed = analysis_data['processed_data']
        quality = analysis_data['quality_metrics']
        metadata = analysis_data['analysis_metadata']
        
        print("âœ… åˆ†æå®Œæˆï¼")
        print(f"   å¤„ç†é¡¹ç›®: {metadata['total_items_processed']} æ¡")
        print(f"   æœ‰æ•ˆé¡¹ç›®: {metadata['valid_items']} æ¡")
        print(f"   å»é‡é¡¹ç›®: {metadata['duplicates_removed']} æ¡")
        
        print(f"\nğŸ“Š æ•°æ®è´¨é‡è¯„ä¼°:")
        print(f"   æ•°æ®å®Œæ•´æ€§: {quality['data_completeness']:.2%}")
        print(f"   æ¥æºå¯é æ€§: {quality['source_reliability']:.2%}")
        print(f"   ä¿¡æ¯æ—¶æ•ˆæ€§: {quality['information_freshness']:.2%}")
        print(f"   æ•´ä½“è´¨é‡: {quality['overall_quality']:.2%}")
        
        print(f"\nğŸ“ æ™ºèƒ½æ‘˜è¦:")
        print(f"   {processed['summary']}")
        
        print(f"\nğŸ† æ¨èæ’è¡Œ (å‰5å):")
        for i, rec in enumerate(processed['top_recommendations'][:5]):
            print(f"   {i+1}. {rec['name']} (è¯„åˆ†: {rec['score']:.2f})")
            print(f"      æ¨èç†ç”±: {', '.join(rec['reasons'])}")
            if rec['practical_info']:
                print(f"      å®ç”¨ä¿¡æ¯: {rec['practical_info']}")
        
        print(f"\nğŸ’¡ å…³é”®æ´å¯Ÿ:")
        for insight in processed['insights']:
            print(f"   â€¢ {insight}")
        
        print(f"\nğŸ“‚ æ•°æ®åˆ†ç±»:")
        for category, items in processed['categories'].items():
            print(f"   {category}: {len(items)} ä¸ªé¡¹ç›®")
    else:
        print(f"âŒ åˆ†æå¤±è´¥: {analysis_data.get('error', 'æœªçŸ¥é”™è¯¯')}")

def test_integration_workflow():
    """æµ‹è¯•å®Œæ•´çš„é›†æˆå·¥ä½œæµç¨‹"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: å®Œæ•´å·¥ä½œæµç¨‹é›†æˆæµ‹è¯•")
    print("=" * 60)
    
    print("\nğŸ”„ æ¨¡æ‹Ÿå®Œæ•´çš„ä¿¡æ¯æŠ“å–å’Œåˆ†ææµç¨‹...")
    
    # æ­¥éª¤1: æŠ“å–å¤šç§ç±»å‹çš„ä¿¡æ¯
    test_queries = [
        {
            'type': 'attractions',
            'query': {
                'location': 'äº¬éƒ½',
                'keywords': ['å¯ºåº™', 'ä¼ ç»Ÿ'],
                'budget_range': [0, 300]
            }
        },
        {
            'type': 'restaurants',
            'query': {
                'location': 'äº¬éƒ½',
                'keywords': ['æ—¥å¼', 'ä¼ ç»Ÿ'],
                'budget_range': [50, 200]
            }
        }
    ]
    
    all_data = []
    for test_query in test_queries:
        print(f"\n   æŠ“å– {test_query['type']} ä¿¡æ¯...")
        result = scrape_travel_info(test_query['type'], test_query['query'])
        data = json.loads(result)
        
        if data['success']:
            all_data.extend(data['raw_data'])
            print(f"   âœ… è·å¾— {len(data['raw_data'])} æ¡ {test_query['type']} ä¿¡æ¯")
        else:
            print(f"   âŒ {test_query['type']} æŠ“å–å¤±è´¥")
    
    if all_data:
        print(f"\n   ğŸ“Š æ€»è®¡è·å¾— {len(all_data)} æ¡ç»¼åˆä¿¡æ¯")
        
        # æ­¥éª¤2: ç»¼åˆåˆ†æ
        print("   ğŸ§  æ‰§è¡Œç»¼åˆåˆ†æ...")
        result = analyze_information(all_data, "comprehensive")
        analysis = json.loads(result)
        
        if analysis['success']:
            print("   âœ… ç»¼åˆåˆ†æå®Œæˆ")
            
            # æ˜¾ç¤ºç»¼åˆæ´å¯Ÿ
            insights = analysis['processed_data']['insights']
            if insights:
                print("   ğŸ’¡ ç»¼åˆæ´å¯Ÿ:")
                for insight in insights[:3]:
                    print(f"      â€¢ {insight}")
            
            # æ˜¾ç¤ºåˆ†ç±»ç»“æœ
            categories = analysis['processed_data']['categories']
            if categories:
                print("   ğŸ“‚ ä¿¡æ¯åˆ†ç±»:")
                for category, items in list(categories.items())[:3]:
                    print(f"      {category}: {len(items)} é¡¹")
        else:
            print("   âŒ ç»¼åˆåˆ†æå¤±è´¥")
    else:
        print("   âŒ æ²¡æœ‰è·å¾—æœ‰æ•ˆæ•°æ®")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ TripMind ç½‘ç»œæŠ“å–å’Œä¿¡æ¯åˆ†æåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    print("æµ‹è¯•ç›®æ ‡:")
    print("  1. Web Scraper Agent - ç½‘ç»œä¿¡æ¯æŠ“å–")
    print("  2. Information Analyzer Agent - ä¿¡æ¯åˆ†æå’Œæ‘˜è¦")
    print("  3. å®Œæ•´å·¥ä½œæµç¨‹é›†æˆ")
    print("=" * 60)
    
    try:
        # æµ‹è¯•1: ç½‘ç»œæŠ“å–
        raw_data = test_web_scraper()
        
        # æµ‹è¯•2: ä¿¡æ¯åˆ†æ
        if raw_data:
            test_information_analyzer(raw_data)
        
        # æµ‹è¯•3: é›†æˆå·¥ä½œæµç¨‹
        test_integration_workflow()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("=" * 60)
        print("âœ… ç½‘ç»œæŠ“å–åŠŸèƒ½æ­£å¸¸")
        print("âœ… ä¿¡æ¯åˆ†æåŠŸèƒ½æ­£å¸¸") 
        print("âœ… å·¥ä½œæµç¨‹é›†æˆæ­£å¸¸")
        print("\nğŸ’¡ æç¤º: ç°åœ¨å¯ä»¥å¯åŠ¨å®Œæ•´çš„ Agent ç³»ç»Ÿè¿›è¡Œå®é™…æµ‹è¯•")
        print("   1. è¿è¡Œ start_network.bat")
        print("   2. è¿è¡Œ start_agents.bat")
        print("   3. åœ¨ Studio ä¸­æµ‹è¯•å®Œæ•´åŠŸèƒ½")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()